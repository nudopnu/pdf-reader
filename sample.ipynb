{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Unit Test Improvement using Large Language Models at Meta  Nadia Alshahwan âˆ—  Jubin Chheda Anastasia Finegenova Beliz Gokkaya Mark Harman Inna Harper Alexandru Marginean Shubho Sengupta Eddy Wang  Meta Platforms Inc., Menlo Park, California, USA  ABSTRACT  This paper describes Metaâ€™s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen- LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLMâ€™s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Metaâ€™s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.  KEYWORDS  Unit Testing, Automated Test Generation, Large Language Models, LLMs, Genetic Improvement.  ACM Reference Format:  Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, and Eddy Wang. 2024. Automated Unit Test Improvement using Large Language Mod- els at Meta. In \n",
      "\n",
      "## Proceedings of the 32nd ACM Symposium on the Foundations\n",
      "\n",
      "## of Software Engineering (FSE â€™24), November 15â€“19, 2024, Porto de Galinhas,\n",
      "\n",
      "## Brazil.\n",
      "\n",
      "ACM, New York, NY, USA, 12 pages. https://doi.org/XXXXXXX. XXXXXXX  âˆ— Author order is alphabetical. The corresponding author is Mark Harman. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  ## FSE â€™24, Mon 15 - Fri 19 July 2024, Porto de Galinhas, Brazil, Brazil\n",
      "\n",
      "Â©  2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX  1  INTRODUCTION  As part of our overall mission to automate unit test generation for Android code, we have developed an automated test class im- prover, TestGen-LLM. TestGen-LLM uses two of Metaâ€™s 1  Large Language Models (LLMs) to extend existing, human-written, Kotlin test classes by generating additional test cases that cover previ- ously missed corner cases, and that increase overall test coverage. TestGen-LLM is an example of Assured Offline LLM-Based Software Engineering (Assured Offline LLMSE) [6]. That is, unlike other LLM-based code and test generation tech- niques, TestGenâ€“LLM uses Assured Offline LLMSE to embed the language models, as a service, in a larger software engineering workflow that ultimately recommends fully formed software im- provements rather than smaller code snippets. These fully-formed code improvements are backed by verifiable guarantees for im- provement and non-regression of existing behavior. A filtration process discards any test case that cannot be guaranteed to meet the assurances. The filtration process can be used to evaluate the performance of a particular LLM, prompt strategy, or choice of hyper-parameters. For this reason, we include telemetry to log the behavior of every execution so that we can evaluate different choices. However, the same infrastructure can also be used as a kind of ensemble learning approach to find test class improvement recommendations. TestGen- LLM thus has two use cases: (1)  Evaluation : To evaluate the effects of different LLMs, prompt- ing strategies, and hyper-parameters on the automatically measurable and verifiable improvements they make to exist- ing code. (2)  Deployment : To fully automate human-independent test class improvement, using a collection of LLMs, prompting strategies, and hyper-parameters to automatically produce code improvement recommendations that are backed by  1 The two LLMs used by TestGen-LLM were constructed at Meta for general purpose internal use, but they are not the focus of this paper, which is about an LLM-agnostic ensemble approach, its application to test class improvement at Meta and our experi- ence with it at Metaâ€™s Test-a-thons. Because details are commercially sensitive (and not relevant to this paper), we do not give details of the two LLMs, simply calling them â€˜LLM1â€™ and â€˜LLM2â€™ in this paper.  arXiv:2402.09171v1 [cs.SE] 14 Feb 2024\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('paper.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Load and parse the JSON data (assumed to be already loaded as `data` from earlier steps)\n",
    "\n",
    "# Initialize Markdown formatted text\n",
    "markdown_text = \"\"\n",
    "\n",
    "# Variables to help with formatting\n",
    "current_paragraph = \"\"\n",
    "last_was_header = False\n",
    "\n",
    "# Process each item in the items list\n",
    "for item in data['items']:\n",
    "    text = item['str'].strip()\n",
    "    font_name = item['fontName']\n",
    "    has_eol = item['hasEOL']\n",
    "\n",
    "    # Determine if the current text is a headline\n",
    "    is_header = (font_name == 'g_d0_f5' or \"Chapter\" in text or \"Section\" in text or \"Key Concepts\" in text)\n",
    "\n",
    "    # Handle headline formatting\n",
    "    if is_header and text:\n",
    "        # If there is an existing paragraph, add it before the header\n",
    "        if current_paragraph:\n",
    "            markdown_text += current_paragraph + \"\\n\\n\"\n",
    "            current_paragraph = \"\"\n",
    "        # Format the header and add to markdown\n",
    "        markdown_text += f\"## {text}\\n\\n\"\n",
    "        last_was_header = True\n",
    "    else:\n",
    "        # Continue building the paragraph\n",
    "        if current_paragraph:\n",
    "            # Add a space if it's a continuation of a paragraph\n",
    "            current_paragraph += \" \" + text if not last_was_header else text\n",
    "        else:\n",
    "            current_paragraph = text\n",
    "        last_was_header = False\n",
    "\n",
    "        # Add paragraph to markdown if end of line is reached\n",
    "        if has_eol and current_paragraph:\n",
    "            markdown_text += current_paragraph + \" \"\n",
    "            current_paragraph = \"\"\n",
    "\n",
    "# Check if there's any remaining paragraph to add\n",
    "if current_paragraph:\n",
    "    markdown_text += current_paragraph + \"\\n\\n\"\n",
    "\n",
    "# Output the Markdown text\n",
    "print(markdown_text.strip())  # Strip any extra newlines at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Unit Test Improvement using Large Language Models\n",
      "at Meta \n",
      "Nadia Alshahwan âˆ— \n",
      "Jubin Chheda\n",
      "Anastasia Finegenova\n",
      "Beliz Gokkaya\n",
      "Mark Harman\n",
      "Inna Harper\n",
      "Alexandru Marginean\n",
      "Shubho Sengupta\n",
      "Eddy Wang \n",
      "Meta Platforms Inc.,\n",
      "Menlo Park, California, USA \n",
      "ABSTRACT \n",
      "This paper describes Metaâ€™s TestGen-LLM tool, which uses LLMs\n",
      "to automatically improve existing human-written tests. TestGen-\n",
      "LLM verifies that its generated test classes successfully clear a set\n",
      "of filters that assure measurable improvement over the original\n",
      "test suite, thereby eliminating problems due to LLM hallucination.\n",
      "We describe the deployment of TestGen-LLM at Meta test-a-thons\n",
      "for the Instagram and Facebook platforms. In an evaluation on\n",
      "Reels and Stories products for Instagram, 75% of TestGen-LLMâ€™s\n",
      "test cases built correctly, 57% passed reliably, and 25% increased\n",
      "coverage. During Metaâ€™s Instagram and Facebook test-a-thons, it\n",
      "improved 11.5% of all classes to which it was applied, with 73% of\n",
      "its recommendations being accepted for production deployment\n",
      "by Meta software engineers. We believe this is the first report on\n",
      "industrial scale deployment of LLM-generated code backed by such\n",
      "assurances of code improvement. \n",
      "KEYWORDS \n",
      "Unit Testing, Automated Test Generation, Large Language Models,\n",
      "LLMs, Genetic Improvement. \n",
      "ACM Reference Format: \n",
      "Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya,\n",
      "Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, and Eddy\n",
      "Wang. 2024. Automated Unit Test Improvement using Large Language Mod-\n",
      "els at Meta. In \n",
      "\n",
      "## Proceedings of the 32nd ACM Symposium on the Foundations\n",
      "\n",
      "## of Software Engineering (FSE â€™24), November 15â€“19, 2024, Porto de Galinhas,\n",
      "\n",
      "## Brazil.\n",
      "\n",
      "ACM, New York, NY, USA, 12 pages. https://doi.org/XXXXXXX.\n",
      "XXXXXXX \n",
      "âˆ— Author order is alphabetical. The corresponding author is Mark Harman.\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org. \n",
      "## FSE â€™24, Mon 15 - Fri 19 July 2024, Porto de Galinhas, Brazil, Brazil\n",
      "\n",
      "Â©  2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-XXXX-X/18/06\n",
      "https://doi.org/XXXXXXX.XXXXXXX \n",
      "1  INTRODUCTION \n",
      "As part of our overall mission to automate unit test generation\n",
      "for Android code, we have developed an automated test class im-\n",
      "prover, TestGen-LLM. TestGen-LLM uses two of Metaâ€™s 1  Large\n",
      "Language Models (LLMs) to extend existing, human-written, Kotlin\n",
      "test classes by generating additional test cases that cover previ-\n",
      "ously missed corner cases, and that increase overall test coverage.\n",
      "TestGen-LLM is an example of Assured Offline LLM-Based Software\n",
      "Engineering (Assured Offline LLMSE) [6].\n",
      "That is, unlike other LLM-based code and test generation tech-\n",
      "niques, TestGenâ€“LLM uses Assured Offline LLMSE to embed the\n",
      "language models, as a service, in a larger software engineering\n",
      "workflow that ultimately recommends fully formed software im-\n",
      "provements rather than smaller code snippets. These fully-formed\n",
      "code improvements are backed by verifiable guarantees for im-\n",
      "provement and non-regression of existing behavior. A filtration\n",
      "process discards any test case that cannot be guaranteed to meet\n",
      "the assurances.\n",
      "The filtration process can be used to evaluate the performance of\n",
      "a particular LLM, prompt strategy, or choice of hyper-parameters.\n",
      "For this reason, we include telemetry to log the behavior of every\n",
      "execution so that we can evaluate different choices. However, the\n",
      "same infrastructure can also be used as a kind of ensemble learning\n",
      "approach to find test class improvement recommendations. TestGen-\n",
      "LLM thus has two use cases:\n",
      "(1)  Evaluation : To evaluate the effects of different LLMs, prompt-\n",
      "ing strategies, and hyper-parameters on the automatically\n",
      "measurable and verifiable improvements they make to exist-\n",
      "ing code.\n",
      "(2)  Deployment : To fully automate human-independent test\n",
      "class improvement, using a collection of LLMs, prompting\n",
      "strategies, and hyper-parameters to automatically produce\n",
      "code improvement recommendations that are backed by \n",
      "1 The two LLMs used by TestGen-LLM were constructed at Meta for general purpose\n",
      "internal use, but they are not the focus of this paper, which is about an LLM-agnostic\n",
      "ensemble approach, its application to test class improvement at Meta and our experi-\n",
      "ence with it at Metaâ€™s Test-a-thons. Because details are commercially sensitive (and\n",
      "not relevant to this paper), we do not give details of the two LLMs, simply calling them\n",
      "â€˜LLM1â€™ and â€˜LLM2â€™ in this paper. \n",
      "arXiv:2402.09171v1 [cs.SE] 14 Feb 2024\n"
     ]
    }
   ],
   "source": [
    "# Updated script for improved Markdown formatting\n",
    "\n",
    "# Initialize Markdown formatted text\n",
    "improved_markdown_text = \"\"\n",
    "\n",
    "# Variables to help with formatting\n",
    "current_paragraph = \"\"\n",
    "last_was_header = False\n",
    "\n",
    "# Process each item in the items list\n",
    "for item in data['items']:\n",
    "    text = item['str'].strip()\n",
    "    font_name = item['fontName']\n",
    "    has_eol = item['hasEOL']\n",
    "\n",
    "    # Determine if the current text is a headline\n",
    "    is_header = (font_name == 'g_d0_f5' or \"Chapter\" in text or \"Section\" in text or \"Key Concepts\" in text)\n",
    "\n",
    "    # Handle headline formatting\n",
    "    if is_header and text:\n",
    "        # If there is an existing paragraph, add it before the header\n",
    "        if current_paragraph:\n",
    "            improved_markdown_text += current_paragraph + \"\\n\\n\"\n",
    "            current_paragraph = \"\"\n",
    "        # Clean up the header to remove unwanted characters and format\n",
    "        cleaned_header = text.replace(\"� \", \"\")  # Remove unwanted symbols\n",
    "        # Format the header and add to markdown\n",
    "        improved_markdown_text += f\"## {cleaned_header}\\n\\n\"\n",
    "        last_was_header = True\n",
    "    else:\n",
    "        # Continue building the paragraph\n",
    "        if current_paragraph:\n",
    "            # Add a space if it's a continuation of a paragraph\n",
    "            current_paragraph += \" \" + text if not last_was_header else text\n",
    "        else:\n",
    "            current_paragraph = text\n",
    "        last_was_header = False\n",
    "\n",
    "        # Add paragraph to markdown if end of line is reached\n",
    "        if has_eol and current_paragraph:\n",
    "            improved_markdown_text += current_paragraph + \"\\n\"\n",
    "            current_paragraph = \"\"\n",
    "\n",
    "# Check if there's any remaining paragraph to add\n",
    "if current_paragraph:\n",
    "    improved_markdown_text += current_paragraph + \"\\n\\n\"\n",
    "\n",
    "# Output the improved Markdown text\n",
    "print(improved_markdown_text.strip())  # Strip any extra newlines at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Unit Test Improvement using Large Language Models at Meta\n",
      "Nadia Alshahwan\n",
      "âˆ—\n",
      "Jubin Chheda\n",
      "Anastasia Finegenova\n",
      "Beliz Gokkaya\n",
      "Mark Harman\n",
      "Inna Harper\n",
      "Alexandru Marginean\n",
      "Shubho Sengupta\n",
      "Eddy Wang \n",
      "Meta Platforms Inc.,\n",
      "Menlo Park, California, USA \n",
      "ABSTRACT \n",
      "This paper describes Metaâ€™s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination.\n",
      "We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLMâ€™s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Metaâ€™s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.\n",
      "KEYWORDS \n",
      "Unit Testing, Automated Test Generation, Large Language Models, LLMs, Genetic Improvement. \n",
      "ACM Reference Format: \n",
      "Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, and Eddy Wang. 2024. Automated Unit Test Improvement using Large Language Mod-els at Meta. In   Proceedings of the 32nd ACM Symposium on the Foundations of Software Engineering (FSE â€™24), November 15â€“19, 2024, Porto de Galinhas, Brazil.   ACM, New York, NY, USA, 12 pages. https://doi.org/XXXXXXX.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def process_json_file(data):    \n",
    "    text_content = []\n",
    "    current_paragraph = \"\"\n",
    "    previous_item = None\n",
    "\n",
    "    for item in data[\"items\"]:\n",
    "        text_str = item[\"str\"]\n",
    "        is_end_of_line = item[\"hasEOL\"]\n",
    "        \n",
    "        # Improved heuristic to determine if consecutive lines should be combined\n",
    "        if previous_item:\n",
    "            vertical_distance = abs(item[\"transform\"][5] - previous_item[\"transform\"][5] + previous_item[\"height\"])\n",
    "            font_size_current = item[\"transform\"][3]\n",
    "            font_size_previous = previous_item[\"transform\"][3]\n",
    "            # Combining lines if they are close vertically and the font size does not change significantly\n",
    "            font_size_difference = abs(font_size_current - font_size_previous)\n",
    "            if vertical_distance < 12 and font_size_difference <= 2:\n",
    "                # Combine with a space if it seems like a continuation of a headline\n",
    "                if not current_paragraph.endswith('-') and current_paragraph != \"\":\n",
    "                    current_paragraph += \" \"\n",
    "                current_paragraph += text_str\n",
    "            else:\n",
    "                if current_paragraph:\n",
    "                    text_content.append(current_paragraph)\n",
    "                current_paragraph = text_str\n",
    "        else:\n",
    "            current_paragraph = text_str  # Initialize the first paragraph\n",
    "        \n",
    "        previous_item = item\n",
    "\n",
    "        # # At end of line, decide if it's a headline\n",
    "        if is_end_of_line and current_paragraph:\n",
    "            if current_paragraph.endswith('.') or len(item['str'].split()) < 5:\n",
    "                text_content.append(current_paragraph)\n",
    "                current_paragraph = \"\"\n",
    "\n",
    "    if current_paragraph:  # Append the last paragraph if any\n",
    "        text_content.append(current_paragraph)\n",
    "\n",
    "    return text_content\n",
    "\n",
    "# Process the JSON data and apply the refined headline detection and merging logic\n",
    "processed_text = process_json_file(data)\n",
    "print(\"\\n\".join(processed_text[:20]))  # Display the first 20 lines to check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
